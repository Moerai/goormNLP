{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"[HW25_Answer] Topic Modeling.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"pUEqzkNmnyYH"},"source":["# Topic Modeling\n","1. Crawling News\n","2. Preprocessing\n","3. Build Term-Document Matrix\n","4. Topic modeling\n","5. Visualization\n","\n","```\n","🔥 이번 시간에는 Topic Modeling를 직접 크롤링한 뉴스 데이터에 대해서 수행해보는 시간을 갖겠습니다. \n","\n","먼저 네이버에서 뉴스 기사를 간단하게 크롤링합니다.\n","기본적인 전처리 이후 Term-document Matrix를 만들고 이를 non-negative factorization을 이용해 행렬 분해를 하여 Topic modeling을 수행합니다.\n","\n","t-distributed stochastic neighbor embedding(T-SNE) 기법을 이용해 Topic별 시각화를 진행합니다.\n","```"]},{"cell_type":"code","metadata":{"id":"NIARcrg_oNMN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632847538719,"user_tz":-540,"elapsed":19794,"user":{"displayName":"Tae Hee Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11953226521466726862"}},"outputId":"9dc10fd6-ddbc-4ac2-ac29-8041d624a3dd"},"source":["!pip install newspaper3k\n","!pip install konlpy"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting newspaper3k\n","  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n","\u001b[?25l\r\u001b[K     |█▌                              | 10 kB 19.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 20 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 30 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 40 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 51 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 61 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 71 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 81 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 102 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 112 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 122 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 133 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 143 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 153 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 163 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 174 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 184 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 194 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 204 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 211 kB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\n","Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.13)\n","Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\n","Collecting cssselect>=0.9.2\n","  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n","Collecting feedfinder2>=0.0.4\n","  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n","Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\n","Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\n","Collecting tldextract>=2.0.1\n","  Downloading tldextract-3.1.2-py2.py3-none-any.whl (87 kB)\n","\u001b[K     |████████████████████████████████| 87 kB 4.5 MB/s \n","\u001b[?25hCollecting tinysegmenter==0.3\n","  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n","Collecting jieba3k>=0.35.1\n","  Downloading jieba3k-0.35.1.zip (7.4 MB)\n","\u001b[K     |████████████████████████████████| 7.4 MB 37.2 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.2)\n","Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5)\n","Collecting feedparser>=5.2.1\n","  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n","\u001b[K     |████████████████████████████████| 81 kB 9.1 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n","Collecting sgmllib3k\n","  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n","Collecting requests-file>=1.4\n","  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n","Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.0.12)\n","Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n","  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13552 sha256=99c7fcf524146b0f36a23104d7cfe4e1487cd3039a9d3da7e984c2939fa15193\n","  Stored in directory: /root/.cache/pip/wheels/df/67/41/faca10fa501ca010be41b49d40360c2959e1c4f09bcbfa37fa\n","  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3356 sha256=95a3991317793990f2e6b4e317d5e6ad27d34fe111ac051fc2f73f9a7eec3c62\n","  Stored in directory: /root/.cache/pip/wheels/7f/d4/8f/6e2ca54744c9d7292d88ddb8d42876bcdab5e6d84a21c10346\n","  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398405 sha256=abd9674dc6eecaad4e6a5fdba7e972388969e18963e19ca21a3f44d3731abd48\n","  Stored in directory: /root/.cache/pip/wheels/4c/91/46/3c208287b726df325a5979574324878b679116e4baae1af3c3\n","  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=b6e684541c3fa1c43ce21c2b909b271990171e9c5c917a6e73c0f64c4794e332\n","  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n","Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n","Installing collected packages: sgmllib3k, requests-file, tldextract, tinysegmenter, jieba3k, feedparser, feedfinder2, cssselect, newspaper3k\n","Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.8 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.1.2\n","Collecting konlpy\n","  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n","\u001b[K     |████████████████████████████████| 19.4 MB 1.2 MB/s \n","\u001b[?25hCollecting colorama\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Collecting beautifulsoup4==4.6.0\n","  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 6.0 MB/s \n","\u001b[?25hCollecting JPype1>=0.7.0\n","  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 64.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n","Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n","Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n","Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n","  Attempting uninstall: beautifulsoup4\n","    Found existing installation: beautifulsoup4 4.6.3\n","    Uninstalling beautifulsoup4-4.6.3:\n","      Successfully uninstalled beautifulsoup4-4.6.3\n","Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"]}]},{"cell_type":"code","metadata":{"id":"aPu9TCFEoK_m","executionInfo":{"status":"ok","timestamp":1632847550174,"user_tz":-540,"elapsed":417,"user":{"displayName":"Tae Hee Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11953226521466726862"}}},"source":["# 크롤링에 필요한 패키지 설치\n","from bs4 import BeautifulSoup\n","from newspaper import Article\n","from time import sleep\n","from time import time\n","from dateutil.relativedelta import relativedelta\n","from datetime import datetime\n","from multiprocessing import Pool\n","import json\n","import requests\n","import re\n","import sys"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oU2ipk6ZvZXf"},"source":["```\n","💡 Crawling(크롤링)이란?\n","\n","크롤링은 웹 페이지에서 필요한 데이터를 추출해내는 작업을 말합니다.\n","이번 시간에는 정적 페이지인 네이버의 뉴스 신문 기사 웹페이지를 크롤링합니다.\n","\n","HTML은 설명되어 있는 자료가 많기 때문에 생략하도록 하겠습니다.\n","HTML 구조 파악 및 태그에 대한 설명은 아래 참고자료를 살펴봐주세요 !\n","```\n","\n","참고: [위키피디아: 정적페이지](https://ko.wikipedia.org/wiki/%EC%A0%95%EC%A0%81_%EC%9B%B9_%ED%8E%98%EC%9D%B4%EC%A7%80)\n","\n","참고: [생활코딩: HTML](https://opentutorials.org/course/2039)"]},{"cell_type":"code","metadata":{"id":"c0UYQWmsnHTF","executionInfo":{"status":"ok","timestamp":1632847556284,"user_tz":-540,"elapsed":422,"user":{"displayName":"Tae Hee Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11953226521466726862"}}},"source":["def crawl_news(query: str=None, crawl_num: int=1000, workers: int=4):\n","    '''뉴스 기사 텍스트가 담긴 list를 반환합니다.\n","\n","    Keyword arguments:\n","    query -- 검색어 (default None)\n","    crawl_num -- 수집할 뉴스 기사의 개수 (defualt 1000)\n","    workers -- multi-processing시 사용할 thread의 개수 (default 4)\n","    '''\n","\n","    url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query={}'\n","    articleList = []\n","    crawled_url = set()\n","    keyboard_interrupt = False\n","    t = time()\n","    idx = 0\n","    page = 1\n","\n","    \n","    # 서버에 url 요청의 결과를 선언\n","    res = requests.get(url.format(query))\n","    sleep(0.5)\n","    # res를 parsing할 parser를 선언\n","    bs = BeautifulSoup(res.text, 'html.parser')\n","    \n","    with Pool(workers) as p:\n","        while idx < crawl_num:            \n","            table = bs.find('ul', {'class': 'list_news'})\n","            li_list = table.find_all('li', {'id': re.compile('sp_nws.*')})\n","            area_list = [li.find('div', {'class':'news_area'}) for li in li_list]\n","            a_list = [area.find('a', {'class':'news_tit'}) for area in area_list]\n","            \n","            for n in a_list[:min(len(a_list), crawl_num-idx)]:\n","                articleList.append(n.get('title'))\n","                idx += 1\n","            page += 1\n","\n","            pages = bs.find('div', {'class': 'sc_page_inner'})\n","            next_page_url = [p for p in pages.find_all('a') if p.text == str(page)][0].get('href')\n","\n","            req = requests.get('https://search.naver.com/search.naver' + next_page_url)\n","            bs = BeautifulSoup(req.text, 'html.parser')\n","    return articleList"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ln1Wih0XVnt"},"source":["```\n","🔥 이제 '구글'이라는 이름으로 뉴스 기사 1000개의 제목을 크롤링하겠습니다.\n","```"]},{"cell_type":"code","metadata":{"id":"G6CAFa6J56yJ","executionInfo":{"status":"ok","timestamp":1632847613990,"user_tz":-540,"elapsed":53601,"user":{"displayName":"Tae Hee Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11953226521466726862"}}},"source":["query = '구글'\n","\n","articleList = crawl_news(query)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"UDIS5V1yAcRT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632847649854,"user_tz":-540,"elapsed":399,"user":{"displayName":"Tae Hee Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11953226521466726862"}},"outputId":"08cc9207-0f4b-4235-e4b0-d25b54aa8a63"},"source":["articleList[:10]"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['구글코리아, 국내 유망 스타트업 지원한다',\n"," '상생방안 서둘러 발표한 애플·구글...\"국감 앞두고 여론 환기용?\"',\n"," '플랫폼 국감… 카카오-네이버-구글코리아-쿠팡 대표 증인 채택',\n"," '구글, 픽셀 태블릿도 선보일까',\n"," 'NHN한국사이버결제, 구글플레이 PG시스템 개통 지연 장기화…김빠진 성과',\n"," '애플, 포스텍에 中企 R&D 지원센터… 구글 “한국 스타트업 지원”',\n"," '구글, 화상회의 실시간 번역 베타테스트 나서...언어장벽 걷힐까',\n"," '카카오모빌리티 국감행…애플·구글 등 IT공룡도 대표 소환',\n"," '공정위, 안드로이드만 강요한 구글에 과징금 | 부당계약으로 경쟁 차단…구글 “항소하겠다”',\n"," '구글·애플은 접고, MS는 붙이고…폴더블폰 경쟁 불붙는다']"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"J9HG4LxOaa1r"},"source":["```\n","🔥 태거(tagger)를 이용해 한글 명사와 알파벳만을 추출해서 term-document matrix (tdm)을 만들겠습니다.\n","\n","태거(tagger)는 tokenization에서 조금 더 자세히 다루도록 하겠습니다.\n","```\n","\n","참고: [konlpy: morph analyzer](https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/)"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2019-05-15T06:17:04.018304Z","start_time":"2019-05-15T06:09:10.729214Z"},"id":"Mdr-aTytaCLr"},"source":["## Preprocessing"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-05-29T08:48:53.790086Z","start_time":"2019-05-29T08:46:38.984818Z"},"id":"HUN9k5DXaCLs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632847667122,"user_tz":-540,"elapsed":14000,"user":{"displayName":"Tae Hee Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11953226521466726862"}},"outputId":"7df4b813-3540-492c-fffd-8b6454e7fe72"},"source":["from konlpy.tag import Okt\n","from collections import Counter\n","import json\n","\n","# Okt 형태소 분석기 선언\n","t = Okt()\n","\n","words_list_ = []\n","vocab = Counter()\n","tag_set = set(['Noun', 'Alpha'])\n","stopwords = set(['글자'])\n","\n","for i, article in enumerate(articleList):\n","    if i % 100 == 0:\n","        print(i)\n","    \n","    # tagger를 이용한 품사 태깅\n","    words = t.pos(article, norm=True, stem=True)\n","\n","    words = [w for w, t in words if t in tag_set and len(w) > 1 and w not in stopwords]\n","    vocab.update(words)\n","    words_list_.append((words, article))\n","    \n","vocab = sorted([w for w, freq in vocab.most_common(10000)])\n","word2id = {w: i for i, w in enumerate(vocab)}\n","words_list = []\n","for words, article in words_list_:\n","    words = [w for w in words if w in word2id]\n","    if len(words) > 10:\n","        words_list.append((words, article))\n","        \n","del words_list_"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","100\n","200\n","300\n","400\n","500\n","600\n","700\n","800\n","900\n"]}]},{"cell_type":"markdown","metadata":{"id":"5L1BkQeaaCLv"},"source":["## Build document-term matrix"]},{"cell_type":"markdown","metadata":{"id":"r5YfokxJf86R"},"source":["```\n","🔥 이제 document-term matrix를 만들어보겠습니다.\n","document-term matrix는 (문서 개수 x 단어 개수)의 Matrix입니다.\n","```\n","\n","참고: [Document-Term Matrix](https://wikidocs.net/24559)"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-05-29T08:48:56.005889Z","start_time":"2019-05-29T08:48:53.792571Z"},"id":"MI5weWREaCLv","executionInfo":{"status":"ok","timestamp":1632847713790,"user_tz":-540,"elapsed":931,"user":{"displayName":"Tae Hee Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11953226521466726862"}}},"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","import numpy as np\n","\n","dtm = np.zeros((len(words_list), len(vocab)), dtype=np.float32)\n","for i, (words, article) in enumerate(words_list):\n","    for word in words:\n","        dtm[i, word2id[word]] += 1\n","        \n","dtm = TfidfTransformer().fit_transform(dtm)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I0xYqpVtmgoL"},"source":["```\n","🔥 document-term matrix를 non-negative factorization(NMF)을 이용해 행렬 분해를 해보겠습니다.\n","\n","💡 Non-negative Factorization이란?\n","\n","NMF는 주어진 행렬 non-negative matrix X를 non-negative matrix W와 H로 행렬 분해하는 알고리즘입니다.\n","이어지는 코드를 통해 W와 H의 의미에 대해 파악해봅시다.\n","```\n","참고: [Non-negative Matrix Factorization](https://angeloyeo.github.io/2020/10/15/NMF.html)"]},{"cell_type":"markdown","metadata":{"id":"zRWNuIguaCLy"},"source":["## Topic modeling"]},{"cell_type":"code","metadata":{"id":"erhwS2ntQBSD","executionInfo":{"status":"ok","timestamp":1632847717691,"user_tz":-540,"elapsed":394,"user":{"displayName":"Tae Hee Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11953226521466726862"}}},"source":["# Non-negative Matrix Factorization\n","from sklearn.decomposition import NMF\n","\n","K=5\n","nmf = NMF(n_components=K, alpha=0.1)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5I24Y78G3VvO"},"source":["```\n","🔥 sklearn의 NMF를 이용해 W와 H matrix를 구해봅시다.\n","W는 document length x K, H는 K x term length의 차원을 갖고 있습니다.\n","W의 하나의 row는 각각의 feature에 얼만큼의 가중치를 줄 지에 대한 weight입니다.\n","H의 하나의 row는 하나의 feature를 나타냅니다.\n","\n","우선 하나의 Topic (H의 n번째 row)에 접근해서 해당 topic에 대해 값이 가장 높은 20개의 단어를 출력해보겠습니다.\n","```"]},{"cell_type":"code","metadata":{"id":"v2TY6gu4QH1o","executionInfo":{"status":"ok","timestamp":1632847720274,"user_tz":-540,"elapsed":404,"user":{"displayName":"Tae Hee Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11953226521466726862"}}},"source":["W = nmf.fit_transform(dtm)\n","H = nmf.components_"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kbNWlTAv5Zn2"},"source":["```\n","🔥 우선 하나의 Topic (H의 n번째 row)에 접근해서 해당 topic에 대해 값이 가장 높은 20개의 단어를 출력해보겠습니다.\n","```"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-05-29T08:48:58.523062Z","start_time":"2019-05-29T08:48:58.500171Z"},"id":"uSzB5PBuaCL2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632847775802,"user_tz":-540,"elapsed":398,"user":{"displayName":"Tae Hee Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11953226521466726862"}},"outputId":"ac0c4957-d93c-4599-dc15-221c9bb24987"},"source":["for k in range(K):\n","    print(f\"{k}th topic\")\n","    for index in H[k].argsort()[::-1][:20]:\n","        print(vocab[index], end=' ')\n","    print()"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["0th topic\n","인앱 결제 애플 방지법 강제 세계 시행 통과 최초 구글 금지법 업계 국회 환영 이행 마켓 금지 계획 IT 난색 \n","1th topic\n","현직 롯데 진행 채용 강남구 특강 공기업 아카데미 청년 취업 멘토링 온라인 구글 정책 호주 워크숍 ACT 교육청 교육감 BYOD \n","2th topic\n","공정위 OS 안드로이드 시장 행위 지위 제재 구글 지배 남용 관련 과징금 혁신 저해 경쟁 플랫폼 강요 항소 스마트 부과 \n","3th topic\n","블랙 캠페인 핑크 예정 아티스트 연설 유일 특별 Earth Dear 퍼포 참가 구글 참여 버락 오바마 어스 프란치스코 디어 교황 \n","4th topic\n","출시 RPG 신스 타임즈 수집 미소녀 버전 소녀 헌터 플레이 한국 이용 구글 픽셀 포트 움직임 게임 효과 에픽 앱스토어 \n"]}]},{"cell_type":"markdown","metadata":{"id":"PTP6lUft5C4j"},"source":["```\n","🔥 이번에는 W에서 하나의 Topic (W의 n번째 column)에 접근해서 해당 topic에 대해 값이 가장 높은 3개의 뉴스 기사 제목을 출력해보겠습니다.\n","```"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-05-29T08:48:58.553639Z","start_time":"2019-05-29T08:48:58.525169Z"},"scrolled":false,"id":"K1CniIn7aCL5"},"source":["for k in range(K):\n","    print(f\"==={k}th topic===\")\n","    for index in W[:, k].argsort()[::-1][:3]:\n","        print(words_list[index][1])\n","    print('\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yLOplJlO5tFi"},"source":["```\n","❓ 2번째 토픽에 대해 가장 높은 가중치를 갖는 제목 5개를 출력해볼까요?\n","```"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-05-29T08:48:58.574010Z","start_time":"2019-05-29T08:48:58.567122Z"},"id":"Jfm4fK4jaCL9"},"source":["#TODO\n","index_list = W[:, 2].argsort()[::-1][:5]\n","for index in index_list:\n","    print(words_list[index][1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b7AZ3SyS6Y5L"},"source":["```\n","🔥 이번에는 t-SNE를 이용해 Topic별 시각화를 진행해보겠습니다.\n","\n","💡 t-SNE는 무엇인가요?\n","\n","t-Stochastic Neighbor Embedding(t-SNE)은 고차원의 벡터를 \n","저차원(2~3차원) 벡터로 데이터간 구조적 특징을 유지하며 축소를 하는 방법 중 하나입니다.\n","\n","주로 고차원 데이터의 시각화를 위해 사용됩니다.\n","```\n","\n","참고: [lovit: t-SNE](https://lovit.github.io/nlp/representation/2018/09/28/tsne/#:~:text=t%2DSNE%20%EB%8A%94%20%EA%B3%A0%EC%B0%A8%EC%9B%90%EC%9D%98,%EC%9D%98%20%EC%A7%80%EB%8F%84%EB%A1%9C%20%ED%91%9C%ED%98%84%ED%95%A9%EB%8B%88%EB%8B%A4.)\n","\n","참고: [ratsgo: t-SNE](https://ratsgo.github.io/machine%20learning/2017/04/28/tSNE/)"]},{"cell_type":"markdown","metadata":{"id":"H1HlyoakaCMA"},"source":["## Visualization"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-05-29T08:49:22.529080Z","start_time":"2019-05-29T08:48:58.612549Z"},"id":"rqajA3lDaCMB"},"source":["from sklearn.manifold import TSNE\n","\n","# n_components = 차원 수\n","tsne = TSNE(n_components=2, init='pca', verbose=1)\n","\n","# W matrix에 대해 t-sne를 수행합니다.\n","W2d = tsne.fit_transform(W)\n","\n","# 각 뉴스 기사 제목마다 가중치가 가장 높은 topic을 저장합니다.\n","topicIndex = [v.argmax() for v in W]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-05-16T08:24:10.840813Z","start_time":"2019-05-16T08:24:10.695706Z"},"scrolled":true,"id":"pc76X-jSaCME"},"source":["from bokeh.models import HoverTool\n","from bokeh.palettes import Category20\n","from bokeh.io import show, output_notebook\n","from bokeh.plotting import figure, ColumnDataSource\n","output_notebook()\n","\n","# 사용할 툴들\n","tools_to_show = 'hover,box_zoom,pan,save,reset,wheel_zoom'\n","p = figure(plot_width=720, plot_height=580, tools=tools_to_show)\n","\n","source = ColumnDataSource(data={\n","    'x': W2d[:, 0],\n","    'y': W2d[:, 1],\n","    'id': [i for i in range(W.shape[0])],\n","    'document': [article for words, article in words_list],\n","    'topic': [str(i) for i in topicIndex],  # 토픽 번호\n","    'color': [Category20[K][i] for i in topicIndex]\n","})\n","p.circle(\n","    'x', 'y',\n","    source=source,\n","    legend='topic',\n","    color='color'\n",")\n","\n","# interaction\n","p.legend.location = \"top_left\"\n","hover = p.select({'type': HoverTool})\n","hover.tooltips = [(\"Topic\", \"@topic\"), ('id', '@id'), (\"Article\", \"@document\")]\n","hover.mode = 'mouse'\n","\n","show(p)"],"execution_count":null,"outputs":[]}]}