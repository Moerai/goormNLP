{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[HW1.2]LRvsMLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR26RFkwXtvi"
      },
      "source": [
        "# **[HW1.2] LR vs MLP**\n",
        "1. Import packages\n",
        "2. Preprocess Dataset\n",
        "3. Pytorch Dataset\n",
        "4. Model\n",
        "5. Trainer\n",
        "\n",
        "PyTorch에서 모델을 학습시키는 과정은 크게 3단계로 나누어져 진행됩니다.\n",
        "1. Initilaize Dataset and DataLoader\n",
        "2. Initialize Model\n",
        "3. Train Model\n",
        "\n",
        "이번 실습에서는 머신러닝 수업때 활용했던 Mnist dataset을 활용하여 logistic regression model과 MLP model을 구현해보고 학습 파이프라인을 익혀보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4iquuOQj1g9"
      },
      "source": [
        "# 1. Import packages\n",
        "\n",
        "> 필요한 package를 설치하고 import합니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpvlE_XOWS33"
      },
      "source": [
        "런타임의 유형을 변경해줍니다.\n",
        "\n",
        "상단 메뉴에서 [런타임]->[런타임유형변경]->[하드웨어가속기]->[GPU]\n",
        "\n",
        "변경 이후 아래의 cell을 실행 시켰을 때, torch.cuda.is_avialable()이 True가 나와야 합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqVdEuPQzMAH"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o3-HPdHLZma"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "np.set_printoptions(precision=3)\n",
        "np.set_printoptions(suppress=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMkIJgfEl9kD"
      },
      "source": [
        "# 2. Preprocess Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8XqtZa8sXsw"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCnmrA9ltYs0"
      },
      "source": [
        "mnist = fetch_openml('mnist_784', cache=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5m4qus2UnoL"
      },
      "source": [
        "mnist.data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BfwipaTYEFI"
      },
      "source": [
        "## Preprocess Dataset\n",
        "\n",
        "Mnist에 존재하는 각각의 image는 28*28의 픽셀로 구성된 784차원 짜리 벡터로 나타나져 있습니다. 각 픽셀은 0-255사이의 값으로 흰색부터 검은색 사이의 값을 나타냅니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upkEJ9mBMCOd"
      },
      "source": [
        "X = mnist.data.astype('float32')\n",
        "y = mnist.target.astype('int64')\n",
        "X = X.values\n",
        "y = y.values\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obZ-A5rxYOSx"
      },
      "source": [
        "값이 너무 커지는 것을 방지하기 위해 [0,255]사이의 input을 [0,1]의 scale로 조정해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkJRGOaEyyc0"
      },
      "source": [
        "X /= 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk1OASeazKtN"
      },
      "source": [
        "print(X.min(), X.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB-u3e9taDjT"
      },
      "source": [
        "## Split Dataset\n",
        "\n",
        "학습과 평가를 위한 dataset으로 나눕니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HWWRcNjaLDi"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5)\n",
        "print(X_train.shape) # 80%\n",
        "print(y_train.shape)\n",
        "print(X_val.shape) # 10%\n",
        "print(y_val.shape)\n",
        "print(X_test.shape) # 10%\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A7LymV5aYEA"
      },
      "source": [
        "## Visualize Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpFTKldSQFiZ"
      },
      "source": [
        "X[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKvYqt3ZaOXZ"
      },
      "source": [
        "def plot_example(X, y):\n",
        "    \"\"\"Plot the first 5 images and their labels in a row.\"\"\"\n",
        "    for i, (img, y) in enumerate(zip(X[:5].reshape(5, 28, 28), y[:5])):\n",
        "        plt.subplot(151 + i)\n",
        "        plt.imshow(img)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.title(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHkBF7cejNC1"
      },
      "source": [
        "plot_example(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYnvqbdijWUQ"
      },
      "source": [
        "# 3. Pytorch Dataset \n",
        "\n",
        "PyTorch에서는 Custom Dataset을 사용하기 위해서는 torch.utils.data.Dataset의 형태로 dataset class를 정의해준 이후, torch.utils.data.DataLoader의 형태로 dataloader class를 정의하여 학습시에 model에 forwarding할 data를 sample해줍니다.\n",
        "\n",
        "(https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75WjNxMrzCOb"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y_5YkXSybCA"
      },
      "source": [
        "가장 보편적으로 사용되는 map-style의 dataset class는 torch.utils.data.Dataset을 superclass로 받아 __getitem__()과 __len__()함수를 override해줍니다.\n",
        "\n",
        "```\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    return self.data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypqp7zA-xRlB"
      },
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        super(CustomDataset, self).__init__()\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.X[index]\n",
        "        y = self.y[index]\n",
        "        x = torch.from_numpy(x).float()\n",
        "        y = torch.from_numpy(np.array(y)).long()\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTr4OWatzmaU"
      },
      "source": [
        "train_dataset = CustomDataset(X_train, y_train)\n",
        "val_dataset = CustomDataset(X_val, y_val)\n",
        "test_dataset = CustomDataset(X_test, y_test)\n",
        "\n",
        "print(len(train_dataset))\n",
        "print(train_dataset.X.shape)\n",
        "print(len(val_dataset))\n",
        "print(val_dataset.X.shape)\n",
        "print(len(test_dataset))\n",
        "print(test_dataset.X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51PT-uPVzE8_"
      },
      "source": [
        "## DataLoader\n",
        "\n",
        "DataLoader는 train 혹은 validation시 dataset에서 batch를 sampling하기 위한 API입니다 (https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n",
        "\n",
        "필수적으로 사용하는 option들은 아래와 같습니다.\n",
        "- dataset: sampling할 dataset\n",
        "- batch_size: 한번에 sampling할 dataset의 개수\n",
        "- shuffle: 1 epoch를 기준으로 dataset을 shuffle할지\n",
        "\n",
        "더 자세한 option이 궁금하시다면 api를 참고해주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2k3YVBoxRnF"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "# shuffle the train data\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# do not shuffle the val & test data\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# dataset size // batch_size\n",
        "print(len(train_dataloader))\n",
        "print(len(val_dataloader))\n",
        "print(len(test_dataloader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN65oTBk1d4T"
      },
      "source": [
        "# 4. Model\n",
        "\n",
        "\n",
        "Pytorch에서 model을 선언할 때는 torch.nn.Module class를 superclass로 받아 __init__()함수와 forward() 함수를 작성해줍니다.\n",
        "\n",
        "__init__()함수에는 모델의 파라미터들을 선언하고, forward함수에는 해당 파라미터들을 이용하여 data를 model에 통과시켜줍니다.\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.Module.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6scAJcC11KR"
      },
      "source": [
        "## Initialize Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9fPzVmH1dZF"
      },
      "source": [
        "class LR(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LR, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3wsp8tJ2w4e"
      },
      "source": [
        "## Initialize MLP Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6_0LrGL1ddk"
      },
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1GnKJCB4T_Q"
      },
      "source": [
        "# 5. Train\n",
        "\n",
        "이제 선언한 model을 통해 학습을 진행하기 위해서는 model의 파라미터를 최적화할 optimizer가 필요합니다. 이번 실습에서는 가장 보편적으로 사용되는 Adam optimizer를 사용하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zPmZhpZlZkQ"
      },
      "source": [
        "### Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPfV0OTc4Xdr"
      },
      "source": [
        "class Trainer():\n",
        "    def __init__(self, trainloader, valloader, testloader, model, optimizer, criterion, device):\n",
        "        \"\"\"\n",
        "        trainloader: train data's loader\n",
        "        testloader: test data's loader\n",
        "        model: model to train\n",
        "        optimizer: optimizer to update your model\n",
        "        criterion: loss function\n",
        "        \"\"\"\n",
        "        self.trainloader = trainloader\n",
        "        self.valloader = valloader\n",
        "        self.testloader = testloader\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.device = device\n",
        "        \n",
        "    def train(self, epoch = 1):\n",
        "        # 학습을 시작할 때 model을 train-mode로 바꿔주어야함.\n",
        "        # Question) 왜 model을 train-mode로 바꿔야 할까요?\n",
        "        self.model.train()\n",
        "        for e in range(epoch):\n",
        "            running_loss = 0.0  \n",
        "            for i, data in enumerate(self.trainloader, 0): \n",
        "                inputs, labels = data \n",
        "                # model에 input으로 tensor를 gpu-device로 보낸다\n",
        "                inputs = inputs.to(self.device)  \n",
        "                labels = labels.to(self.device)\n",
        "                # zero the parameter gradients\n",
        "                # Question) 왜 optimizer.zero_grad()라는 함수를 사용해야 할까요?\n",
        "                self.optimizer.zero_grad()    \n",
        "                # forward + backward + optimize\n",
        "                # get output after passing through the network\n",
        "                outputs = self.model(inputs) \n",
        "                # compute model's score using the loss function\n",
        "                loss = self.criterion(outputs, labels)  \n",
        "                # perform back-propagation from the loss\n",
        "                loss.backward() \n",
        "                # gradient descent를 통해 model의 output을 얻는다.\n",
        "                self.optimizer.step() \n",
        "                running_loss += loss.item()\n",
        "            \n",
        "            print('epoch: %d  loss: %.3f' % (e + 1, running_loss / len(self.trainloader)))\n",
        "            running_loss = 0.0\n",
        "        val_acc = self.validate()\n",
        "        return val_acc\n",
        "\n",
        "    def validate(self):\n",
        "        # Question) 왜 model을 eval-mode로 바꿔야 할까요?\n",
        "        self.model.eval() \n",
        "        correct = 0\n",
        "        for inputs, labels in self.valloader:\n",
        "            inputs = inputs.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            output = self.model(inputs) \n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max \n",
        "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        return correct / len(self.valloader.dataset)\n",
        "        \n",
        "    def test(self):\n",
        "        self.model.eval() \n",
        "        correct = 0\n",
        "        for inputs, labels in self.testloader:\n",
        "            inputs = inputs.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            output = self.model(inputs) \n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max \n",
        "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        return correct / len(self.testloader.dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKxP2nzvVC_O"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gRfskIWWQEf"
      },
      "source": [
        "input_dim = 784\n",
        "output_dim = 10\n",
        "epoch = 4\n",
        "device = torch.device('cuda')\n",
        "\n",
        "best_acc = 0.0\n",
        "lrs = [1e-1, 1e-2, 1e-3, 1e-4]\n",
        "for lr in lrs:\n",
        "    model = LR(input_dim=input_dim, output_dim=output_dim).to(device)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    trainer = Trainer(train_dataloader, val_dataloader, test_dataloader, model, optimizer, criterion, device)\n",
        "    val_acc = trainer.train(epoch = epoch)\n",
        "    print('val_acc: %.3f' %(val_acc))\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), './best_model')\n",
        "\n",
        "trainer.model.load_state_dict(torch.load('./best_model'))\n",
        "test_acc = trainer.test()\n",
        "print('test_acc: %.3f' %(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vraDZVtolBSa"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht3K2k_miraK"
      },
      "source": [
        "input_dim = 784\n",
        "hidden_dim = 32\n",
        "output_dim = 10\n",
        "epoch = 4\n",
        "device = torch.device('cuda')\n",
        "\n",
        "best_acc = 0.0\n",
        "lrs = [1e-1, 1e-2, 1e-3, 1e-4]\n",
        "for lr in lrs:\n",
        "    model = MLP(input_dim=input_dim, \n",
        "                hidden_dim=hidden_dim,\n",
        "                output_dim=output_dim).to(device)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    trainer = Trainer(train_dataloader, val_dataloader, test_dataloader, model, optimizer, criterion, device)\n",
        "    val_acc = trainer.train(epoch = epoch)\n",
        "    print('val_acc: %.3f' %(val_acc))\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), './best_model')\n",
        "\n",
        "trainer.model.load_state_dict(torch.load('./best_model'))\n",
        "test_acc = trainer.test()\n",
        "print('test_acc: %.3f' %(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPeWLRy6lLHa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}